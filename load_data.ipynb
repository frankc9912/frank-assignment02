{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed file written to: ../data/hw2/bus/stops_preprocessed.txt\n",
      "Loaded preprocessed bus stops into septa.bus_stops\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "def clean_value(val, target_type='text'):\n",
    "    \"\"\"\n",
    "    Cleans a single value for SQL insertion based on the expected target type.\n",
    "    - Converts NA, null, blank to appropriate defaults\n",
    "    - Supports text, int, float (numeric)\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        val = ''\n",
    "    val = val.strip()\n",
    "\n",
    "    # Treat these as missing\n",
    "    if val.lower() in {'', 'na', 'null'}:\n",
    "        if target_type == 'int':\n",
    "            return '0'\n",
    "        elif target_type == 'float':\n",
    "            return '0.0'\n",
    "        else:  # default is text\n",
    "            return ''\n",
    "    \n",
    "    # For actual values, enforce basic type casting (optional safety)\n",
    "    if target_type == 'int':\n",
    "        try:\n",
    "            return str(int(float(val)))\n",
    "        except:\n",
    "            return '0'\n",
    "    elif target_type == 'float':\n",
    "        try:\n",
    "            return str(float(val))\n",
    "        except:\n",
    "            return '0.0'\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "\n",
    "# Step 1: Preprocess\n",
    "input_file = '../data/hw2/bus/stops.txt'\n",
    "output_file = '../data/hw2/bus/stops_preprocessed.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\n",
    "        'stop_id', 'stop_code', 'stop_name', 'stop_desc', 'stop_lat', 'stop_lon',\n",
    "        'zone_id', 'stop_url', 'location_type', 'parent_station', 'stop_timezone', 'wheelchair_boarding'\n",
    "    ]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        writer.writerow({\n",
    "            'stop_id': clean_value(row.get('stop_id')),\n",
    "            'stop_code': clean_value(row.get('stop_code')),\n",
    "            'stop_name': clean_value(row.get('stop_name')),\n",
    "            'stop_desc': clean_value(row.get('stop_desc')),\n",
    "            'stop_lat': clean_value(row.get('stop_lat'), target_type='float'),\n",
    "            'stop_lon': clean_value(row.get('stop_lon'), target_type='float'),\n",
    "            'zone_id': clean_value(row.get('zone_id')),\n",
    "            'stop_url': clean_value(row.get('stop_url')),\n",
    "            'location_type': clean_value(row.get('location_type'), target_type='int'),\n",
    "            'parent_station': clean_value(row.get('parent_station')),\n",
    "            'stop_timezone': clean_value(row.get('stop_timezone')),\n",
    "            'wheelchair_boarding': clean_value(row.get('wheelchair_boarding'), target_type='int')\n",
    "        })\n",
    "\n",
    "print(\"Preprocessed file written to:\", output_file)\n",
    "\n",
    "# Step 2: Load into PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname='hw2',\n",
    "    user='postgres',\n",
    "    password='123456',\n",
    "    host='localhost',\n",
    "    port='5432'\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS septa;\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS septa.bus_stops (\n",
    "            stop_id TEXT,\n",
    "            stop_code TEXT,\n",
    "            stop_name TEXT,\n",
    "            stop_desc TEXT,\n",
    "            stop_lat DOUBLE PRECISION,\n",
    "            stop_lon DOUBLE PRECISION,\n",
    "            zone_id TEXT,\n",
    "            stop_url TEXT,\n",
    "            location_type INTEGER,\n",
    "            parent_station TEXT,\n",
    "            stop_timezone TEXT,\n",
    "            wheelchair_boarding INTEGER\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        cur.copy_expert(\n",
    "            \"COPY septa.bus_stops FROM STDIN WITH CSV HEADER DELIMITER ','\",\n",
    "            f\n",
    "        )\n",
    "    print(\"Loaded preprocessed bus stops into septa.bus_stops\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed routes saved to: ../data/hw2/bus/routes_preprocessed.txt\n",
      "Loaded data into septa.bus_routes\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and write cleaned file\n",
    "input_file = '../data/hw2/bus/routes.txt'\n",
    "output_file = '../data/hw2/bus/routes_preprocessed.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\n",
    "        'route_id', 'agency_id', 'route_short_name', 'route_long_name',\n",
    "        'route_desc', 'route_type', 'route_url', 'route_color', 'route_text_color'\n",
    "    ]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        writer.writerow({\n",
    "            'route_id': clean_value(row.get('route_id')),\n",
    "            'agency_id': clean_value(row.get('agency_id')),\n",
    "            'route_short_name': clean_value(row.get('route_short_name')),\n",
    "            'route_long_name': clean_value(row.get('route_long_name')),\n",
    "            'route_desc': clean_value(row.get('route_desc')),\n",
    "            'route_type': clean_value(row.get('route_type'), target_type='int'),\n",
    "            'route_url': clean_value(row.get('route_url')),\n",
    "            'route_color': clean_value(row.get('route_color')),\n",
    "            'route_text_color': clean_value(row.get('route_text_color')),\n",
    "        })\n",
    "\n",
    "print(\"Preprocessed routes saved to:\", output_file)\n",
    "\n",
    "# Create table and load data\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS septa;\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS septa.bus_routes (\n",
    "            route_id TEXT,\n",
    "            agency_id TEXT,\n",
    "            route_short_name TEXT,\n",
    "            route_long_name TEXT,\n",
    "            route_desc TEXT,\n",
    "            route_type TEXT,\n",
    "            route_url TEXT,\n",
    "            route_color TEXT,\n",
    "            route_text_color TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        cur.copy_expert(\n",
    "            \"COPY septa.bus_routes FROM STDIN WITH CSV HEADER DELIMITER ','\",\n",
    "            f\n",
    "        )\n",
    "    print(\"Loaded data into septa.bus_routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed trips saved to: ../data/hw2/bus/trips_preprocessed.txt\n",
      "Loaded data into septa.bus_trips\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess trips.txt\n",
    "input_file = '../data/hw2/bus/trips.txt'\n",
    "output_file = '../data/hw2/bus/trips_preprocessed.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\n",
    "        'route_id', 'service_id', 'trip_id', 'trip_headsign',\n",
    "        'trip_short_name', 'direction_id', 'block_id', 'shape_id',\n",
    "        'wheelchair_accessible', 'bikes_allowed'\n",
    "    ]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        writer.writerow({\n",
    "            'route_id': clean_value(row.get('route_id')),\n",
    "            'service_id': clean_value(row.get('service_id')),\n",
    "            'trip_id': clean_value(row.get('trip_id')),\n",
    "            'trip_headsign': clean_value(row.get('trip_headsign')),\n",
    "            'trip_short_name': clean_value(row.get('trip_short_name')),\n",
    "            'direction_id': clean_value(row.get('direction_id')),\n",
    "            'block_id': clean_value(row.get('block_id')),\n",
    "            'shape_id': clean_value(row.get('shape_id')),\n",
    "            'wheelchair_accessible': clean_value(row.get('wheelchair_accessible'), target_type='int'),\n",
    "            'bikes_allowed': clean_value(row.get('bikes_allowed'), target_type='int'),\n",
    "        })\n",
    "\n",
    "print(\"Preprocessed trips saved to:\", output_file)\n",
    "\n",
    "# Step 2: Create table and load into DB\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS septa;\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS septa.bus_trips (\n",
    "            route_id TEXT,\n",
    "            service_id TEXT,\n",
    "            trip_id TEXT,\n",
    "            trip_headsign TEXT,\n",
    "            trip_short_name TEXT,\n",
    "            direction_id TEXT,\n",
    "            block_id TEXT,\n",
    "            shape_id TEXT,\n",
    "            wheelchair_accessible INTEGER,\n",
    "            bikes_allowed INTEGER\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        cur.copy_expert(\n",
    "            \"COPY septa.bus_trips FROM STDIN WITH CSV HEADER DELIMITER ','\",\n",
    "            f\n",
    "        )\n",
    "    print(\"Loaded data into septa.bus_trips\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed shapes saved to: ../data/hw2/bus/shapes_preprocessed.txt\n",
      "Loaded data into septa.bus_shapes\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess shapes.txt\n",
    "input_file = '../data/hw2/bus/shapes.txt'\n",
    "output_file = '../data/hw2/bus/shapes_preprocessed.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\n",
    "        'shape_id',\n",
    "        'shape_pt_lat',\n",
    "        'shape_pt_lon',\n",
    "        'shape_pt_sequence',\n",
    "        'shape_dist_traveled'\n",
    "    ]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        writer.writerow({\n",
    "            'shape_id': clean_value(row.get('shape_id')),\n",
    "            'shape_pt_lat': clean_value(row.get('shape_pt_lat'), target_type='float'),\n",
    "            'shape_pt_lon': clean_value(row.get('shape_pt_lon'), target_type='float'),\n",
    "            'shape_pt_sequence': clean_value(row.get('shape_pt_sequence'), target_type='int'),\n",
    "            'shape_dist_traveled': clean_value(row.get('shape_dist_traveled'), target_type='float'),\n",
    "        })\n",
    "\n",
    "print(\"Preprocessed shapes saved to:\", output_file)\n",
    "\n",
    "# Step 2: Create table and load data\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS septa;\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS septa.bus_shapes (\n",
    "            shape_id TEXT,\n",
    "            shape_pt_lat DOUBLE PRECISION,\n",
    "            shape_pt_lon DOUBLE PRECISION,\n",
    "            shape_pt_sequence INTEGER,\n",
    "            shape_dist_traveled DOUBLE PRECISION\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        cur.copy_expert(\n",
    "            \"COPY septa.bus_shapes FROM STDIN WITH CSV HEADER DELIMITER ','\",\n",
    "            f\n",
    "        )\n",
    "    print(\"Loaded data into septa.bus_shapes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed rail stops saved to: ../data/hw2/rail/stops_preprocessed.txt\n",
      "Loaded data into septa.rail_stops\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess rail/stops.txt\n",
    "input_file = '../data/hw2/rail/stops.txt'\n",
    "output_file = '../data/hw2/rail/stops_preprocessed.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\n",
    "        'stop_id',\n",
    "        'stop_name',\n",
    "        'stop_desc',\n",
    "        'stop_lat',\n",
    "        'stop_lon',\n",
    "        'zone_id',\n",
    "        'stop_url'\n",
    "    ]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        writer.writerow({\n",
    "            'stop_id': clean_value(row.get('stop_id')),\n",
    "            'stop_name': clean_value(row.get('stop_name')),\n",
    "            'stop_desc': clean_value(row.get('stop_desc')),\n",
    "            'stop_lat': clean_value(row.get('stop_lat'), target_type='float'),\n",
    "            'stop_lon': clean_value(row.get('stop_lon'), target_type='float'),\n",
    "            'zone_id': clean_value(row.get('zone_id')),\n",
    "            'stop_url': clean_value(row.get('stop_url')),\n",
    "        })\n",
    "\n",
    "print(\"Preprocessed rail stops saved to:\", output_file)\n",
    "\n",
    "# Step 2: Create table and load data\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS septa;\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS septa.rail_stops (\n",
    "            stop_id TEXT,\n",
    "            stop_name TEXT,\n",
    "            stop_desc TEXT,\n",
    "            stop_lat DOUBLE PRECISION,\n",
    "            stop_lon DOUBLE PRECISION,\n",
    "            zone_id TEXT,\n",
    "            stop_url TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        cur.copy_expert(\n",
    "            \"COPY septa.rail_stops FROM STDIN WITH CSV HEADER DELIMITER ','\",\n",
    "            f\n",
    "        )\n",
    "    print(\"Loaded data into septa.rail_stops\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phl.pwd_parcels successfully loaded with GEOGRAPHY column 'geog'\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine, text\n",
    "from geoalchemy2 import Geography\n",
    "import os\n",
    "\n",
    "# Load original GeoJSON and reproject from EPSG:2272 to EPSG:4326\n",
    "gdf = gpd.read_file(\"../data/hw2/PWD_PARCELS.geojson\", engine=\"fiona\")\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Rename geometry column to 'geog' and lowercase all field names\n",
    "gdf = gdf.set_geometry(\"geometry\").rename_geometry(\"geog\")\n",
    "gdf.columns = [col.lower() for col in gdf.columns]\n",
    "\n",
    "# Build connection string\n",
    "DB = {\n",
    "    \"user\":     os.getenv(\"DB_USER\",     \"postgres\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"123456\"),\n",
    "    \"host\":     os.getenv(\"DB_HOST\",     \"localhost\"),\n",
    "    \"port\":     os.getenv(\"DB_PORT\",     \"5432\"),\n",
    "    \"database\": os.getenv(\"DB_NAME\",     \"hw2\"),\n",
    "}\n",
    "conn_str = (\n",
    "    f\"postgresql+psycopg2://\"\n",
    "    f\"{DB['user']}:{DB['password']}@\"\n",
    "    f\"{DB['host']}:{DB['port']}/{DB['database']}\"\n",
    ")\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Ensure PostGIS extension and schema\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS postgis;\"))\n",
    "    conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS phl;\"))\n",
    "\n",
    "# Upload to PostGIS using GEOGRAPHY(MULTIPOLYGON, 4326)\n",
    "gdf.to_postgis(\n",
    "    name=\"pwd_parcels\",\n",
    "    con=engine,\n",
    "    schema=\"phl\",\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype={\"geog\": Geography(\"MULTIPOLYGON\", srid=4326)}\n",
    ")\n",
    "\n",
    "print(\"phl.pwd_parcels successfully loaded with GEOGRAPHY column 'geog'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phl.neighborhoods successfully loaded with geography column 'geog'\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine, text\n",
    "from geoalchemy2 import Geography\n",
    "import os\n",
    "\n",
    "# Step 1: Read GeoJSON\n",
    "gdf = gpd.read_file(\"../data/hw2/philadelphia-neighborhoods.geojson\", engine=\"fiona\")\n",
    "\n",
    "# Step 2: Reproject to EPSG:4326 (optional safety — most GeoJSON already is)\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Step 3: Rename geometry column + lowercase field names\n",
    "gdf = gdf.set_geometry(\"geometry\").rename_geometry(\"geog\")\n",
    "gdf.columns = [col.lower() for col in gdf.columns]\n",
    "\n",
    "# Step 4: Connect to database\n",
    "DB = {\n",
    "    \"user\":     os.getenv(\"DB_USER\",     \"postgres\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"123456\"),\n",
    "    \"host\":     os.getenv(\"DB_HOST\",     \"localhost\"),\n",
    "    \"port\":     os.getenv(\"DB_PORT\",     \"5432\"),\n",
    "    \"database\": os.getenv(\"DB_NAME\",     \"hw2\"),\n",
    "}\n",
    "conn_str = (\n",
    "    f\"postgresql+psycopg2://\"\n",
    "    f\"{DB['user']}:{DB['password']}@\"\n",
    "    f\"{DB['host']}:{DB['port']}/{DB['database']}\"\n",
    ")\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Step 5: Ensure PostGIS + schema\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS postgis;\"))\n",
    "    conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS phl;\"))\n",
    "\n",
    "# Step 6: Upload to PostGIS with GEOGRAPHY\n",
    "gdf.to_postgis(\n",
    "    name=\"neighborhoods\",\n",
    "    con=engine,\n",
    "    schema=\"phl\",\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype={\"geog\": Geography(\"MULTIPOLYGON\", srid=4326)}\n",
    ")\n",
    "\n",
    "print(\"phl.neighborhoods successfully loaded with geography column 'geog'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "census.blockgroups_2020 successfully loaded with geography column 'geog'\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine, text\n",
    "from geoalchemy2 import Geography\n",
    "import os\n",
    "\n",
    "# Step 1: Read the Census shapefile using Fiona\n",
    "gdf = gpd.read_file(\"../data/hw2/tl_2020_42_bg.shp\", engine=\"fiona\")\n",
    "\n",
    "# Step 2: Reproject from EPSG:4269 (NAD83) to EPSG:4326 (WGS 84)\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Step 3: Standardize geometry column + lowercase all fields\n",
    "gdf = gdf.set_geometry(\"geometry\").rename_geometry(\"geog\")\n",
    "gdf.columns = [col.lower() for col in gdf.columns]\n",
    "\n",
    "# Step 4: Create database connection string from env vars (or default fallback)\n",
    "DB = {\n",
    "    \"user\":     os.getenv(\"DB_USER\",     \"postgres\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"123456\"),\n",
    "    \"host\":     os.getenv(\"DB_HOST\",     \"localhost\"),\n",
    "    \"port\":     os.getenv(\"DB_PORT\",     \"5432\"),\n",
    "    \"database\": os.getenv(\"DB_NAME\",     \"hw2\"),\n",
    "}\n",
    "conn_str = (\n",
    "    f\"postgresql+psycopg2://\"\n",
    "    f\"{DB['user']}:{DB['password']}@\"\n",
    "    f\"{DB['host']}:{DB['port']}/{DB['database']}\"\n",
    ")\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Step 5: Ensure PostGIS and schema exist\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS postgis;\"))\n",
    "    conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS census;\"))\n",
    "\n",
    "# Step 6: Upload to PostGIS using GEOGRAPHY(MULTIPOLYGON, 4326)\n",
    "gdf.to_postgis(\n",
    "    name=\"blockgroups_2020\",\n",
    "    con=engine,\n",
    "    schema=\"census\",\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype={\"geog\": Geography(\"MULTIPOLYGON\", srid=4326)}\n",
    ")\n",
    "\n",
    "print(\"census.blockgroups_2020 successfully loaded with geography column 'geog'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded population data into census.population_2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fang_\\AppData\\Local\\Temp\\ipykernel_16584\\53240181.py:6: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_df = pd.read_csv(\"../data/hw2/DECENNIALPL2020.P1-Data.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "# Step 1: Load the full CSV and preview columns\n",
    "raw_df = pd.read_csv(\"../data/hw2/DECENNIALPL2020.P1-Data.csv\")\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "# Usually, GEO_ID is full geoid; NAME is the geography name; P1_001N is total pop.\n",
    "df = pd.DataFrame()\n",
    "df['geoid'] = raw_df['GEO_ID'].str.extract(r'US(\\d+)$')  # Strip \"US\" prefix\n",
    "df['geoname'] = raw_df['NAME']\n",
    "df['total'] = raw_df['P1_001N']  # This is total population\n",
    "\n",
    "# Optional sanity check\n",
    "df = df.dropna(subset=['geoid', 'geoname', 'total'])\n",
    "\n",
    "# Step 3: Save to temporary clean CSV for COPY\n",
    "output_file = \"../data/hw2/population_2020_clean.csv\"\n",
    "df.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"hw2\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "# Step 4: Load into PostgreSQL\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS census;\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS census.population_2020 (\n",
    "            geoid TEXT,\n",
    "            geoname TEXT,\n",
    "            total INTEGER\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        cur.copy_expert(\n",
    "            \"COPY census.population_2020 FROM STDIN WITH CSV HEADER\",\n",
    "            f\n",
    "        )\n",
    "    print(\"Loaded population data into census.population_2020\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
